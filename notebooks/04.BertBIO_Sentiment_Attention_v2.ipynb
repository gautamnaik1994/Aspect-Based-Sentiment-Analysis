{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a975d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0439105",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"O\": 0, \"B-ASP\": 1, \"I-ASP\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "sentiment2id = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1,\n",
    "    \"neutral\": 2\n",
    "}\n",
    "id2sentiment = {v: k for k, v in sentiment2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cc887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pl.read_parquet(\"../data/processed/df_aspect_pos.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763c6f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'attention_mask',\n",
       " 'labels',\n",
       " 'aspects_index',\n",
       " 'aspects_sentiment',\n",
       " 'type']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1fdda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "\n",
    "    aspects_index = [item[\"aspects_index\"] for item in batch]          # List[List[List[int]]]\n",
    "    aspects_sentiment = [item[\"aspects_sentiment\"] for item in batch]  # List[List[int]]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"aspects_index\": aspects_index,\n",
    "        \"aspects_sentiment\": aspects_sentiment,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef7f8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3], [5, 6]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_aspect_spans(pred_labels):\n",
    "    spans = []\n",
    "    i = 0\n",
    "    while i < len(pred_labels):\n",
    "        if (pred_labels[i] == 1):  # B-ASP\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < len(pred_labels) and pred_labels[i] == 2:  # I-ASP\n",
    "                i += 1\n",
    "            end = i - 1\n",
    "            spans.append([start, end])\n",
    "        else:\n",
    "            i += 1\n",
    "    return spans\n",
    "\n",
    "extract_aspect_spans([0, 1, 2, 2, 0, 1, 2, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239b3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d3d111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df[\"input_ids\"].to_numpy()\n",
    "        self.attention_mask = df[\"attention_mask\"].to_numpy()\n",
    "        self.labels = df[\"labels\"].to_numpy()\n",
    "        self.aspects_index = df[\"aspects_index\"].to_list()\n",
    "        self.aspects_sentiment = df[\"aspects_sentiment\"].to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            \"aspects_index\": self.aspects_index[idx],           # list of [start, end]\n",
    "            \"aspects_sentiment\": self.aspects_sentiment[idx],   # list of sentiment values\n",
    "        }\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"train\"))\n",
    "val_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"val\"))\n",
    "test_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"test\"))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size , collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15b8f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "[[0], [1, 1], [0], [1], [1], [1], [1], [1, 1], [0], [0, 0], [1, 1, 1], [1], [2], [1], [1, 1], [0], [1, 1, 1], [1, 1, 1], [1], [2], [1], [1], [0], [1], [1], [2], [0], [1, 1], [1], [1, 1], [1], [0]]\n",
      "[[[9, 9]], [[18, 18], [22, 22]], [[5, 5]], [[1, 2]], [[7, 24]], [[1, 2]], [[5, 6]], [[3, 6], [10, 11]], [[2, 8]], [[2, 3], [9, 9]], [[4, 6], [9, 11], [18, 18]], [[2, 2]], [[13, 13]], [[6, 6]], [[5, 6], [12, 18]], [[6, 7]], [[2, 2], [5, 5], [8, 8]], [[17, 18], [25, 26], [1, 2]], [[4, 6]], [[1, 1]], [[5, 9]], [[5, 6]], [[6, 6]], [[8, 8]], [[2, 2]], [[14, 15]], [[5, 5]], [[4, 5], [23, 24]], [[1, 4]], [[5, 6], [20, 21]], [[2, 2]], [[13, 13]]]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    print(batch[\"attention_mask\"].shape)\n",
    "    print(batch[\"labels\"].shape)\n",
    "    print(batch[\"aspects_sentiment\"])\n",
    "    print(batch[\"aspects_index\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2367963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        # self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.norm = nn.LayerNorm(hidden_size*2)\n",
    "        # self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(hidden_size*2, 3)  # Sentiment classes: pos, neg, neutral\n",
    "        self.attention_param = nn.Linear(hidden_size,1)\n",
    "\n",
    "    def forward(self, token_embeddings, aspect_mask):\n",
    "        # token_embeddings: [B, L, H], aspect_mask: [B, L]\n",
    "\n",
    "        cls_embedding = token_embeddings[:, 0, :]\n",
    "\n",
    "        expaned_aspect_mask = aspect_mask.unsqueeze(-1).expand_as(token_embeddings)  # [B, L, H]\n",
    "        aspect_embeddings = token_embeddings * expaned_aspect_mask  # Zero out non-aspect tokens\n",
    "\n",
    "        attention_score = self.attention_param(aspect_embeddings).squeeze(-1) # [B, L]\n",
    "        attention_score = attention_score.masked_fill(aspect_mask == 0, -1e9) # Mask non-aspect tokens\n",
    "        attention_score = torch.softmax(attention_score, dim=1) # [B, L]\n",
    "\n",
    "        aspect_pooled = torch.bmm(attention_score.unsqueeze(1), token_embeddings).squeeze(1)  # [B, H]\n",
    "        combined = torch.concat([aspect_pooled, cls_embedding ], dim=1)\n",
    "\n",
    "        combined =  self.dropout(combined)\n",
    "        combined = self.norm(combined)  # [B, H]\n",
    "\n",
    "        logits = self.classifier(combined)  # [B, 3]\n",
    "        return logits , attention_score\n",
    "\n",
    "\n",
    "class AspectDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AspectDetectionModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.token_classifier = nn.Linear(self.bert.config.hidden_size, len(label2id))\n",
    "        self.sentiment_classifier = SentimentClassifier(hidden_size=self.bert.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)  # [B, L, H]\n",
    "\n",
    "        token_logits = self.token_classifier(sequence_output)  # For aspect term tagging (BIO)\n",
    "        return token_logits, sequence_output\n",
    "\n",
    "model = AspectDetectionModel().to(\"mps\")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optimizer = torch.optim.AdamW([\n",
    "    {'params': model.bert.parameters(), 'lr': 2e-5},\n",
    "    {'params': model.token_classifier.parameters(), 'lr': 5e-5},\n",
    "    {'params': model.sentiment_classifier.parameters(), 'lr': 5e-5}\n",
    "])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b20ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Aspect Loss: 0.3279, Train Sentiment Loss: 0.5020, Val Aspect Loss: 0.1874, Val Sentiment Loss: 0.3935\n",
      "Epoch [2/20], Train Aspect Loss: 0.1599, Train Sentiment Loss: 0.2909, Val Aspect Loss: 0.1425, Val Sentiment Loss: 0.3990\n",
      "Epoch [3/20], Train Aspect Loss: 0.1199, Train Sentiment Loss: 0.1952, Val Aspect Loss: 0.1302, Val Sentiment Loss: 0.3750\n",
      "Epoch [4/20], Train Aspect Loss: 0.0997, Train Sentiment Loss: 0.1322, Val Aspect Loss: 0.1200, Val Sentiment Loss: 0.4561\n",
      "Epoch [5/20], Train Aspect Loss: 0.0865, Train Sentiment Loss: 0.0855, Val Aspect Loss: 0.1279, Val Sentiment Loss: 0.4734\n",
      "Epoch [6/20], Train Aspect Loss: 0.0753, Train Sentiment Loss: 0.0652, Val Aspect Loss: 0.1201, Val Sentiment Loss: 0.5021\n",
      "Epoch [7/20], Train Aspect Loss: 0.0656, Train Sentiment Loss: 0.0421, Val Aspect Loss: 0.1251, Val Sentiment Loss: 0.6318\n",
      "Epoch [8/20], Train Aspect Loss: 0.0606, Train Sentiment Loss: 0.0344, Val Aspect Loss: 0.1234, Val Sentiment Loss: 0.5849\n",
      "Epoch [9/20], Train Aspect Loss: 0.0529, Train Sentiment Loss: 0.0261, Val Aspect Loss: 0.1300, Val Sentiment Loss: 0.6205\n",
      "Epoch [10/20], Train Aspect Loss: 0.0477, Train Sentiment Loss: 0.0217, Val Aspect Loss: 0.1316, Val Sentiment Loss: 0.5798\n",
      "Epoch [11/20], Train Aspect Loss: 0.0436, Train Sentiment Loss: 0.0168, Val Aspect Loss: 0.1383, Val Sentiment Loss: 0.6595\n",
      "Epoch [12/20], Train Aspect Loss: 0.0400, Train Sentiment Loss: 0.0173, Val Aspect Loss: 0.1414, Val Sentiment Loss: 0.7527\n",
      "Epoch [13/20], Train Aspect Loss: 0.0362, Train Sentiment Loss: 0.0183, Val Aspect Loss: 0.1406, Val Sentiment Loss: 0.6825\n",
      "Epoch [14/20], Train Aspect Loss: 0.0330, Train Sentiment Loss: 0.0156, Val Aspect Loss: 0.1507, Val Sentiment Loss: 0.6549\n",
      "Epoch [15/20], Train Aspect Loss: 0.0321, Train Sentiment Loss: 0.0127, Val Aspect Loss: 0.1472, Val Sentiment Loss: 0.7381\n",
      "Epoch [16/20], Train Aspect Loss: 0.0294, Train Sentiment Loss: 0.0104, Val Aspect Loss: 0.1491, Val Sentiment Loss: 0.6655\n",
      "Epoch [17/20], Train Aspect Loss: 0.0270, Train Sentiment Loss: 0.0126, Val Aspect Loss: 0.1553, Val Sentiment Loss: 0.7024\n",
      "Epoch [18/20], Train Aspect Loss: 0.0253, Train Sentiment Loss: 0.0156, Val Aspect Loss: 0.1588, Val Sentiment Loss: 0.6852\n",
      "Epoch [19/20], Train Aspect Loss: 0.0248, Train Sentiment Loss: 0.0135, Val Aspect Loss: 0.1556, Val Sentiment Loss: 0.6500\n",
      "Epoch [20/20], Train Aspect Loss: 0.0233, Train Sentiment Loss: 0.0103, Val Aspect Loss: 0.1634, Val Sentiment Loss: 0.7056\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 12\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_aspect_train_loss = 0\n",
    "    total_sentiment_train_loss = 0\n",
    "    total_aspect_val_loss = 0\n",
    "    total_sentiment_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"mps\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"mps\")\n",
    "        labels = batch[\"labels\"].to(\"mps\")\n",
    "\n",
    "\n",
    "        token_logits, sequence_output = model(input_ids, attention_mask)\n",
    "        aspect_loss = criterion(token_logits.view(-1, len(label2id)), labels.view(-1))\n",
    "        total_aspect_train_loss += aspect_loss.item()\n",
    "\n",
    "\n",
    "      \n",
    "        sentiment_losses = []\n",
    "        for i in range(len(input_ids)):\n",
    "            for aspect_index, sentiment in zip(batch[\"aspects_index\"][i], batch[\"aspects_sentiment\"][i]):\n",
    "                if aspect_index[1] >= sequence_output.size(1):\n",
    "                    continue\n",
    "                aspect_mask = torch.zeros_like(input_ids, dtype=torch.float).to(\"mps\")\n",
    "                aspect_mask[i, aspect_index[0]:aspect_index[1]+1] = 1\n",
    "                sentiment_logits, _ = model.sentiment_classifier(\n",
    "                    sequence_output[i].unsqueeze(0), aspect_mask[i].unsqueeze(0)\n",
    "                )\n",
    "                sentiment_target = torch.tensor([sentiment], dtype=torch.long).to(\"mps\")\n",
    "                sentiment_loss = criterion(sentiment_logits, sentiment_target)\n",
    "                sentiment_losses.append(sentiment_loss)\n",
    "\n",
    "        if sentiment_losses:\n",
    "            sentiment_loss = torch.stack(sentiment_losses).mean()\n",
    "        else:\n",
    "            sentiment_loss = torch.tensor(0.0).to(\"mps\")\n",
    "\n",
    "        total_sentiment_train_loss += sentiment_loss.item()\n",
    "        total_loss = aspect_loss + sentiment_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # break\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"mps\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"mps\")\n",
    "            labels = batch[\"labels\"].to(\"mps\")\n",
    "\n",
    "            token_logits, sequence_output = model(input_ids, attention_mask)\n",
    "            aspect_loss = criterion(token_logits.view(-1, len(label2id)), labels.view(-1))\n",
    "            total_aspect_val_loss += aspect_loss.item()\n",
    "\n",
    "            preds = torch.argmax(token_logits, dim=2)\n",
    "            sentiment_losses = []\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                predicted_aspects = extract_aspect_spans(preds[i].cpu().tolist())\n",
    "\n",
    "                for aspect_index, sentiment in zip(batch[\"aspects_index\"][i], batch[\"aspects_sentiment\"][i]):\n",
    "                    if aspect_index in predicted_aspects and aspect_index[1] < sequence_output.size(1):\n",
    "                        aspect_mask = torch.zeros_like(input_ids, dtype=torch.float).to(\"mps\")\n",
    "                        aspect_mask[i, aspect_index[0]:aspect_index[1]+1] = 1\n",
    "                        sentiment_logits, _ = model.sentiment_classifier(\n",
    "                            sequence_output[i].unsqueeze(0), aspect_mask[i].unsqueeze(0)\n",
    "                        )\n",
    "                        sentiment_target = torch.tensor([sentiment], dtype=torch.long).to(\"mps\")\n",
    "                        sentiment_loss = criterion(sentiment_logits, sentiment_target)\n",
    "                        sentiment_losses.append(sentiment_loss)\n",
    "\n",
    "            if sentiment_losses:\n",
    "                sentiment_loss = torch.stack(sentiment_losses).mean()\n",
    "            else:\n",
    "                sentiment_loss = torch.tensor(0.0).to(\"mps\")\n",
    "\n",
    "            total_sentiment_val_loss += sentiment_loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "        f\"Train Aspect Loss: {total_aspect_train_loss/len(train_dataloader):.4f}, \"\n",
    "        f\"Train Sentiment Loss: {total_sentiment_train_loss/len(train_dataloader):.4f}, \"\n",
    "        f\"Val Aspect Loss: {total_aspect_val_loss/len(val_dataloader):.4f}, \"\n",
    "        f\"Val Sentiment Loss: {total_sentiment_val_loss/len(val_dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to ../models/AspectDetectionModel/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "name=\"AspectDetectionModel_Sentiment_Analysis_Attention_v2\"\n",
    "\n",
    "os.makedirs(\"../models/AspectDetectionModel\", exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/AspectDetectionModel/\" + name + \".pth\")\n",
    "\n",
    "model_config = {\n",
    "\t\"hidden_size\": model.bert.config.hidden_size,\n",
    "\t\"num_labels\": len(label2id),\n",
    "\t\"id2label\": id2label,\n",
    "\t\"label2id\": label2id,\n",
    "\t\"name\": name\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"../models/AspectDetectionModel/{name}_config.json\", \"w\") as f:\n",
    "\tjson.dump(model_config, f)\n",
    "\n",
    "print(\"Model saved successfully to ../models/AspectDetectionModel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c62198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AspectDetectionModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (token_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (sentiment_classifier): SentimentClassifier(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (classifier): Linear(in_features=1536, out_features=3, bias=True)\n",
       "    (attention_param): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=\"AspectDetectionModel_Sentiment_Analysis_Attention_v2\"\n",
    "model = AspectDetectionModel()\n",
    "model.load_state_dict(torch.load(\"../models/AspectDetectionModel/\" + name + \".pth\"))\n",
    "model = model.to(\"mps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6188977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 6556, 2491, 2482, 2003, 4569, 1010, 3435, 1010, 1998, 3733, 2000, 5047, 1517, 3819, 2005, 4268, 999, 1996, 3857, 3737, 2003, 23073, 1998, 2009, 3216, 15299, 2006, 2367, 9972, 1012, 6046, 2166, 2003, 11519, 1998, 1996, 7711, 2024, 2200, 26651, 1012, 1037, 2307, 5592, 2005, 4268, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "temp = tokenizer((\"\"\"This remote control car is fun, fast, and easy to handle—perfect for kids! The build quality is sturdy and it runs smoothly on different surfaces. Battery life is decent and the controls are very responsive. A great gift for kids!\"\"\").split(), is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47061dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2152, 7597, 2017, 1005, 2128, 2183, 2000, 3477, 2003, 2005, 1996, 3193, 2025, 2005, 1996, 2833, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "temp = tokenizer((\"\"\"the high prices you ' re going to pay is for the view not for the food .\"\"\").split(), is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28521159",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = torch.tensor(temp[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(temp[\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    logits, sequence_output = model(input_ids, attention_mask)\n",
    "    # decode the logits to get the predicted labels\n",
    "    preds = torch.argmax(logits, dim=2)[0]\n",
    "    aspects = extract_aspect_spans(preds.cpu().tolist())\n",
    "    sentiments=[]\n",
    "    for aspect in aspects:\n",
    "        aspect_mask = torch.zeros_like(input_ids, dtype=torch.long).to(\"mps\")\n",
    "        aspect_mask[0, aspect[0]:aspect[1]+1] = 1\n",
    "        # print(\"am\", aspect_mask[0].unsqueeze(0))\n",
    "        sentiment_logits, _ = model.sentiment_classifier(\n",
    "            sequence_output, aspect_mask\n",
    "        )\n",
    "        sentiments.append({\"pos\":aspect,\"senti\":torch.argmax(sentiment_logits, dim=1).item()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000e9f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pos': [3, 3], 'senti': 0}, {'pos': [17, 17], 'senti': 0}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534a6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[-1] * 128\n",
    "arr\n",
    "\n",
    "for item in sentiments:\n",
    "    pos=item[\"pos\"]\n",
    "    start = pos[0]\n",
    "    end = pos[0]+pos[1]\n",
    "    arr[start:end] = [item[\"senti\"]] * (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "838e4123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 3], [17, 17]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3fdf2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [id2label[label] for label in preds.tolist()]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a91b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BIO Tag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b0b64218-01a3-4522-8bd2-ddad646c27cd",
       "rows": [
        [
         "0",
         "O",
         "the",
         ""
        ],
        [
         "1",
         "O",
         "high",
         ""
        ],
        [
         "2",
         "B-ASP",
         "prices",
         "negative"
        ],
        [
         "3",
         "O",
         "you",
         ""
        ],
        [
         "4",
         "O",
         "'",
         ""
        ],
        [
         "5",
         "O",
         "re",
         ""
        ],
        [
         "6",
         "O",
         "going",
         ""
        ],
        [
         "7",
         "O",
         "to",
         ""
        ],
        [
         "8",
         "O",
         "pay",
         ""
        ],
        [
         "9",
         "O",
         "is",
         ""
        ],
        [
         "10",
         "O",
         "for",
         ""
        ],
        [
         "11",
         "O",
         "the",
         ""
        ],
        [
         "12",
         "O",
         "view",
         ""
        ],
        [
         "13",
         "O",
         "not",
         ""
        ],
        [
         "14",
         "O",
         "for",
         ""
        ],
        [
         "15",
         "O",
         "the",
         ""
        ],
        [
         "16",
         "B-ASP",
         "food",
         "negative"
        ],
        [
         "17",
         "O",
         ".",
         ""
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 18
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIO Tag</th>\n",
       "      <th>Word</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>high</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-ASP</td>\n",
       "      <td>prices</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>you</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O</td>\n",
       "      <td>re</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O</td>\n",
       "      <td>going</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O</td>\n",
       "      <td>pay</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O</td>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>O</td>\n",
       "      <td>view</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>O</td>\n",
       "      <td>not</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>O</td>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B-ASP</td>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>O</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BIO Tag    Word Sentiment\n",
       "0        O     the          \n",
       "1        O    high          \n",
       "2    B-ASP  prices  negative\n",
       "3        O     you          \n",
       "4        O       '          \n",
       "5        O      re          \n",
       "6        O   going          \n",
       "7        O      to          \n",
       "8        O     pay          \n",
       "9        O      is          \n",
       "10       O     for          \n",
       "11       O     the          \n",
       "12       O    view          \n",
       "13       O     not          \n",
       "14       O     for          \n",
       "15       O     the          \n",
       "16   B-ASP    food  negative\n",
       "17       O       .          "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_sentiment(bio_tag, sentiment):\n",
    "    if(bio_tag in [\"B-ASP\", \"I-ASP\"]):\n",
    "        return id2sentiment[sentiment]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for item in zip(predictions, temp[\"input_ids\"], arr):\n",
    "    word = tokenizer.convert_ids_to_tokens(item[1])\n",
    "    if word in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "        continue\n",
    "    sentiment = print_sentiment(item[0], item[2])\n",
    "    rows.append({\n",
    "        \"BIO Tag\": item[0],\n",
    "        \"Word\": word,\n",
    "        \"Sentiment\": sentiment\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57d866c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_html(index=False).replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc06e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('remote control car', (2, 4)),\n",
       " ('build quality', (20, 21)),\n",
       " ('battery life', (32, 33)),\n",
       " ('controls', (38, 38))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_aspects(input_ids, predictions, tokenizer):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    aspects = []\n",
    "    current_aspect = []\n",
    "    current_position = []\n",
    "    current_idx = 0\n",
    "\n",
    "    for idx, (token, label) in enumerate(zip(tokens, predictions)):\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            continue\n",
    "        label_tag = label\n",
    "        \n",
    "        if label_tag == \"B-ASP\":\n",
    "            if current_aspect:\n",
    "                aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "            current_aspect = [token]\n",
    "            current_position = [idx]\n",
    "        elif label_tag == \"I-ASP\" and current_aspect:\n",
    "            current_aspect.append(token)\n",
    "            current_position.append(idx)\n",
    "        else:\n",
    "            if current_aspect:\n",
    "                aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "                current_aspect = []\n",
    "                current_position = []\n",
    "\n",
    "    if current_aspect:\n",
    "        aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "\n",
    "    clean_aspects = []\n",
    "    for aspect, pos in aspects:\n",
    "        cleaned = \"\"\n",
    "        for word in aspect.split():\n",
    "            if word.startswith(\"##\"):\n",
    "                cleaned += word[2:]  # Remove ## prefix\n",
    "            else:\n",
    "                if cleaned:\n",
    "                    cleaned += \" \"\n",
    "                cleaned += word\n",
    "        clean_aspects.append((cleaned, pos))\n",
    "\n",
    "    return clean_aspects\n",
    "\n",
    "\n",
    "extracted_aspects = extract_aspects(input_ids, predictions, tokenizer)\n",
    "extracted_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74184c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
