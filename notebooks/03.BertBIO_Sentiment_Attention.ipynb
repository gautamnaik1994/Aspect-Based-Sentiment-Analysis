{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a975d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0439105",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"O\": 0, \"B-ASP\": 1, \"I-ASP\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22cc887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pl.read_parquet(\"../data/processed/df_aspect_pos.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb82704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763c6f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'attention_mask',\n",
       " 'labels',\n",
       " 'aspects_index',\n",
       " 'aspects_sentiment',\n",
       " 'type']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "\n",
    "    aspects_index = [item[\"aspects_index\"] for item in batch]          # List[List[List[int]]]\n",
    "    aspects_sentiment = [item[\"aspects_sentiment\"] for item in batch]  # List[List[int]]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"aspects_index\": aspects_index,\n",
    "        \"aspects_sentiment\": aspects_sentiment,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef7f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspect_spans(pred_labels):\n",
    "    spans = []\n",
    "    i = 0\n",
    "    while i < len(pred_labels):\n",
    "        if pred_labels[i] == 1:  # B-ASP\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < len(pred_labels) and pred_labels[i] == 2:  # I-ASP\n",
    "                i += 1\n",
    "            end = i - 1\n",
    "            spans.append([start, end])\n",
    "        else:\n",
    "            i += 1\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3d111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df[\"input_ids\"].to_numpy()\n",
    "        self.attention_mask = df[\"attention_mask\"].to_numpy()\n",
    "        self.labels = df[\"labels\"].to_numpy()\n",
    "        self.aspects_index = df[\"aspects_index\"].to_list()\n",
    "        self.aspects_sentiment = df[\"aspects_sentiment\"].to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            \"aspects_index\": self.aspects_index[idx],           # list of [start, end]\n",
    "            \"aspects_sentiment\": self.aspects_sentiment[idx],   # list of sentiment values\n",
    "        }\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"train\"))\n",
    "val_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"val\"))\n",
    "test_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"test\"))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size , collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15b8f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "[[1], [1], [0], [1], [1], [1], [0, 1], [1], [0], [1, 1, 1], [1], [0], [2], [1, 1], [1, 1, 1], [1, 1, 1], [1], [1], [0], [1], [1, 1], [1], [1], [1], [1], [0], [1], [1], [1], [1, 1], [1], [1]]\n",
      "[[[2, 2]], [[15, 15]], [[7, 7]], [[2, 3]], [[13, 16]], [[4, 4]], [[2, 3], [7, 8]], [[7, 7]], [[2, 4]], [[7, 7], [10, 10], [15, 15]], [[2, 3]], [[11, 11]], [[6, 8]], [[2, 2], [8, 9]], [[1, 9], [18, 19], [39, 40]], [[2, 2], [5, 6], [9, 9]], [[13, 13]], [[2, 3]], [[6, 7]], [[8, 10]], [[3, 4], [11, 14]], [[2, 2]], [[4, 8]], [[2, 2]], [[2, 2]], [[5, 5]], [[2, 2]], [[1, 1]], [[2, 3]], [[2, 2], [17, 17]], [[2, 2]], [[4, 5]]]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    print(batch[\"attention_mask\"].shape)\n",
    "    print(batch[\"labels\"].shape)\n",
    "    print(batch[\"aspects_sentiment\"])\n",
    "    print(batch[\"aspects_index\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2367963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, 3)  # Sentiment classes: pos, neg, neutral\n",
    "\n",
    "    def forward(self, token_embeddings, aspect_mask):\n",
    "        # token_embeddings: [B, L, H], aspect_mask: [B, L]\n",
    "        aspect_mask = aspect_mask.unsqueeze(-1).expand_as(token_embeddings)  # [B, L, H]\n",
    "        aspect_embeddings = token_embeddings * aspect_mask  # Zero out non-aspect tokens\n",
    "\n",
    "        aspect_pooled = aspect_embeddings.sum(dim=1) / (aspect_mask.sum(dim=1) + 1e-8)  # [B, H]\n",
    "\n",
    "        query = aspect_pooled.unsqueeze(1)  # [B, 1, H]\n",
    "        key = value = token_embeddings  # [B, L, H]\n",
    "\n",
    "        attended_output, attn_weights = self.attention(query, key, value)  # [B, 1, H]\n",
    "        attended_output = self.dropout(attended_output)\n",
    "        attended_output = self.norm(attended_output)\n",
    "\n",
    "        logits = self.classifier(attended_output.squeeze(1))  # [B, 3]\n",
    "        return logits, attn_weights \n",
    "\n",
    "\n",
    "class AspectDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AspectDetectionModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.token_classifier = nn.Linear(self.bert.config.hidden_size, len(label2id))\n",
    "        self.sentiment_classifier = SentimentClassifier(hidden_size=self.bert.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)  # [B, L, H]\n",
    "\n",
    "        token_logits = self.token_classifier(sequence_output)  # For aspect term tagging (BIO)\n",
    "\n",
    "        return token_logits, sequence_output\n",
    "\n",
    "model = AspectDetectionModel().to(\"mps\")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b20ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Aspect Loss: 0.3742, Train Sentiment Loss: 0.5146, Val Aspect Loss: 0.2219, Val Sentiment Loss: 0.3944\n",
      "Epoch [2/20], Train Aspect Loss: 0.1975, Train Sentiment Loss: 0.3532, Val Aspect Loss: 0.1700, Val Sentiment Loss: 0.3869\n",
      "Epoch [3/20], Train Aspect Loss: 0.1503, Train Sentiment Loss: 0.2818, Val Aspect Loss: 0.1405, Val Sentiment Loss: 0.4164\n",
      "Epoch [4/20], Train Aspect Loss: 0.1240, Train Sentiment Loss: 0.2235, Val Aspect Loss: 0.1275, Val Sentiment Loss: 0.3928\n",
      "Epoch [5/20], Train Aspect Loss: 0.1118, Train Sentiment Loss: 0.1878, Val Aspect Loss: 0.1218, Val Sentiment Loss: 0.4012\n",
      "Epoch [6/20], Train Aspect Loss: 0.1021, Train Sentiment Loss: 0.1619, Val Aspect Loss: 0.1172, Val Sentiment Loss: 0.4071\n",
      "Epoch [7/20], Train Aspect Loss: 0.0930, Train Sentiment Loss: 0.1262, Val Aspect Loss: 0.1168, Val Sentiment Loss: 0.4575\n",
      "Epoch [8/20], Train Aspect Loss: 0.0867, Train Sentiment Loss: 0.1029, Val Aspect Loss: 0.1146, Val Sentiment Loss: 0.4285\n",
      "Epoch [9/20], Train Aspect Loss: 0.0814, Train Sentiment Loss: 0.0934, Val Aspect Loss: 0.1163, Val Sentiment Loss: 0.4483\n",
      "Epoch [10/20], Train Aspect Loss: 0.0772, Train Sentiment Loss: 0.0788, Val Aspect Loss: 0.1139, Val Sentiment Loss: 0.4552\n",
      "Epoch [11/20], Train Aspect Loss: 0.0719, Train Sentiment Loss: 0.0670, Val Aspect Loss: 0.1161, Val Sentiment Loss: 0.5136\n",
      "Epoch [12/20], Train Aspect Loss: 0.0697, Train Sentiment Loss: 0.0578, Val Aspect Loss: 0.1179, Val Sentiment Loss: 0.5347\n",
      "Epoch [13/20], Train Aspect Loss: 0.0654, Train Sentiment Loss: 0.0503, Val Aspect Loss: 0.1178, Val Sentiment Loss: 0.5240\n",
      "Epoch [14/20], Train Aspect Loss: 0.0626, Train Sentiment Loss: 0.0453, Val Aspect Loss: 0.1181, Val Sentiment Loss: 0.5355\n",
      "Epoch [15/20], Train Aspect Loss: 0.0604, Train Sentiment Loss: 0.0409, Val Aspect Loss: 0.1174, Val Sentiment Loss: 0.5145\n",
      "Epoch [16/20], Train Aspect Loss: 0.0573, Train Sentiment Loss: 0.0401, Val Aspect Loss: 0.1187, Val Sentiment Loss: 0.5324\n",
      "Epoch [17/20], Train Aspect Loss: 0.0572, Train Sentiment Loss: 0.0366, Val Aspect Loss: 0.1185, Val Sentiment Loss: 0.5414\n",
      "Epoch [18/20], Train Aspect Loss: 0.0551, Train Sentiment Loss: 0.0338, Val Aspect Loss: 0.1180, Val Sentiment Loss: 0.5622\n",
      "Epoch [19/20], Train Aspect Loss: 0.0535, Train Sentiment Loss: 0.0296, Val Aspect Loss: 0.1207, Val Sentiment Loss: 0.5768\n",
      "Epoch [20/20], Train Aspect Loss: 0.0526, Train Sentiment Loss: 0.0269, Val Aspect Loss: 0.1214, Val Sentiment Loss: 0.5933\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_aspect_train_loss = 0\n",
    "    total_sentiment_train_loss = 0\n",
    "    total_aspect_val_loss = 0\n",
    "    total_sentiment_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"mps\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"mps\")\n",
    "        labels = batch[\"labels\"].to(\"mps\")\n",
    "\n",
    "\n",
    "        token_logits, sequence_output = model(input_ids, attention_mask)\n",
    "        aspect_loss = criterion(token_logits.view(-1, len(label2id)), labels.view(-1))\n",
    "        total_aspect_train_loss += aspect_loss.item()\n",
    "\n",
    "\n",
    "      \n",
    "        sentiment_losses = []\n",
    "        for i in range(len(input_ids)):\n",
    "            for aspect_index, sentiment in zip(batch[\"aspects_index\"][i], batch[\"aspects_sentiment\"][i]):\n",
    "                if aspect_index[1] >= sequence_output.size(1):\n",
    "                    continue\n",
    "                # Create a new aspect mask for each aspect instead of modifying in-place\n",
    "                aspect_mask = torch.zeros_like(input_ids, dtype=torch.float).to(\"mps\")\n",
    "                aspect_mask[i, aspect_index[0]:aspect_index[1]+1] = 1\n",
    "                sentiment_logits, _ = model.sentiment_classifier(sequence_output[i].unsqueeze(0), aspect_mask[i].unsqueeze(0))\n",
    "                sentiment_target = torch.tensor([sentiment], dtype=torch.long).to(\"mps\")\n",
    "                sentiment_loss = criterion(sentiment_logits, sentiment_target)\n",
    "                sentiment_losses.append(sentiment_loss)\n",
    "\n",
    "\n",
    "        if sentiment_losses:\n",
    "            sentiment_loss = torch.stack(sentiment_losses).mean()\n",
    "        else:\n",
    "            sentiment_loss = torch.tensor(0.0).to(\"mps\")\n",
    "\n",
    "        total_sentiment_train_loss += sentiment_loss.item()\n",
    "        total_loss = aspect_loss + sentiment_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"mps\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"mps\")\n",
    "            labels = batch[\"labels\"].to(\"mps\")\n",
    "\n",
    "            token_logits, sequence_output = model(input_ids, attention_mask)\n",
    "            aspect_loss = criterion(token_logits.view(-1, len(label2id)), labels.view(-1))\n",
    "            total_aspect_val_loss += aspect_loss.item()\n",
    "\n",
    "            sentiment_losses = []\n",
    "            preds = torch.argmax(token_logits, dim=2)\n",
    "            for i in range(len(input_ids)):\n",
    "                aspects = extract_aspect_spans(preds[i].cpu().tolist())\n",
    "                for aspect_index, sentiment in zip(batch[\"aspects_index\"][i], batch[\"aspects_sentiment\"][i]):\n",
    "                    if aspect_index in aspects and aspect_index[1] < sequence_output.size(1):\n",
    "                        aspect_mask = torch.zeros_like(input_ids, dtype=torch.float).to(\"mps\")\n",
    "                        aspect_mask[i, aspect_index[0]:aspect_index[1]+1] = 1\n",
    "                        sentiment_logits, _ = model.sentiment_classifier(sequence_output[i].unsqueeze(0), aspect_mask[i].unsqueeze(0))\n",
    "                        sentiment_target = torch.tensor([sentiment], dtype=torch.long).to(\"mps\")\n",
    "                        sentiment_loss = criterion(sentiment_logits.view(-1, 3), sentiment_target)\n",
    "                        sentiment_losses.append(sentiment_loss)\n",
    "\n",
    "            if sentiment_losses:\n",
    "                sentiment_loss = torch.stack(sentiment_losses).mean()\n",
    "            else:\n",
    "                sentiment_loss = torch.tensor(0.0).to(\"mps\")\n",
    "\n",
    "            total_sentiment_val_loss += sentiment_loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "        f\"Train Aspect Loss: {total_aspect_train_loss/len(train_dataloader):.4f}, \"\n",
    "        f\"Train Sentiment Loss: {total_sentiment_train_loss/len(train_dataloader):.4f}, \"\n",
    "        f\"Val Aspect Loss: {total_aspect_val_loss/len(val_dataloader):.4f}, \"\n",
    "        f\"Val Sentiment Loss: {total_sentiment_val_loss/len(val_dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to ../models/AspectDetectionModel/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "name=\"AspectDetectionModel_Sentiment_Analysis_Attention\"\n",
    "\n",
    "os.makedirs(\"../models/AspectDetectionModel\", exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/AspectDetectionModel/\" + name + \".pth\")\n",
    "\n",
    "model_config = {\n",
    "\t\"hidden_size\": model.bert.config.hidden_size,\n",
    "\t\"num_labels\": len(label2id),\n",
    "\t\"id2label\": id2label,\n",
    "\t\"label2id\": label2id,\n",
    "\t\"name\": name\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"../models/AspectDetectionModel/{name}_config.json\", \"w\") as f:\n",
    "\tjson.dump(model_config, f)\n",
    "\n",
    "print(\"Model saved successfully to ../models/AspectDetectionModel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c62198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AspectDetectionModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (sentiment_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=\"AspectDetectionModel_Sentiment_Analysis\"\n",
    "model = AspectDetectionModel()\n",
    "model.load_state_dict(torch.load(\"../models/AspectDetectionModel/\" + name + \".pth\"))\n",
    "model = model.to(\"mps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee4af7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2482, 3737, 2003, 2200, 3835, 2021, 1996, 11486, 19237, 1012, 1996, 11486, 1997, 2023, 2482, 2079, 2025, 2573, 7919, 1998, 1996, 2345, 1999, 1996, 11486, 2079, 2025, 24357, 3929, 2009, 2069, 24357, 2066, 6462, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "temp = tokenizer((\"\"\"Car quality is very nice but the controller sucks . The controller of this car do not works properly and the final in the controller do not rotate fully it only rotate like button\"\"\").split(), is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28521159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], device='mps:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"mps\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    \n",
    "    input_ids = torch.tensor(temp[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(temp[\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    logits, sequence_output = model(input_ids, attention_mask)\n",
    "    # decode the logits to get the predicted labels\n",
    "    preds = torch.argmax(logits, dim=2)[0]\n",
    "    aspects = extract_aspect_spans(preds.cpu().tolist())\n",
    "    sentiments=[]\n",
    "    for aspect in aspects:\n",
    "        aspect_mask = torch.zeros_like(input_ids, dtype=torch.long).to(\"mps\")\n",
    "        aspect_mask[0, aspect[0]:aspect[1]+1] = 1\n",
    "        # print(\"am\", aspect_mask[0].unsqueeze(0))\n",
    "        sentiment_logits, _ = model.sentiment_classifier(\n",
    "            sequence_output, aspect_mask\n",
    "        )\n",
    "        sentiments.append({\"pos\":aspect,\"senti\":torch.argmax(sentiment_logits, dim=1).item()})\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "000e9f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.5587, -2.3159, -2.7290],\n",
       "         [ 0.2546,  1.6434, -2.5836],\n",
       "         [ 0.9176, -1.5676,  0.9227],\n",
       "         [ 4.8864, -2.4453, -2.7059],\n",
       "         [ 5.2138, -2.4290, -3.0259],\n",
       "         [ 5.0780, -2.0911, -2.9120],\n",
       "         [ 5.2390, -2.5481, -3.3259],\n",
       "         [ 5.2350, -2.8260, -3.0732],\n",
       "         [-0.1817,  2.9009, -2.4859],\n",
       "         [ 5.2310, -2.3744, -2.7506],\n",
       "         [ 6.3429, -3.2683, -3.3029],\n",
       "         [ 4.1683, -1.5477, -2.7579],\n",
       "         [-0.6483,  3.1019, -2.4083],\n",
       "         [ 2.2763, -2.9142,  0.4111],\n",
       "         [ 5.5308, -2.7043, -2.8574],\n",
       "         [ 2.8816, -0.6622, -3.0009],\n",
       "         [ 5.4649, -2.4530, -2.8229],\n",
       "         [ 5.4646, -2.3330, -2.7968],\n",
       "         [ 5.4757, -2.2137, -3.1635],\n",
       "         [ 5.4349, -2.2264, -3.1097],\n",
       "         [ 5.2646, -2.4777, -2.9180],\n",
       "         [ 4.5705, -2.3099, -2.4537],\n",
       "         [ 2.7997, -0.4281, -2.0433],\n",
       "         [ 3.4136, -1.8345, -0.6538],\n",
       "         [ 4.3592, -3.6476, -1.4186],\n",
       "         [ 1.5128,  0.3037, -2.6704],\n",
       "         [ 5.1675, -2.4397, -2.8870],\n",
       "         [ 5.3524, -2.5540, -2.8283],\n",
       "         [ 5.2359, -1.9360, -3.0754],\n",
       "         [ 5.2549, -2.3690, -3.2337],\n",
       "         [ 4.8655, -2.1251, -2.6278],\n",
       "         [ 4.7861, -1.9486, -2.6694],\n",
       "         [ 4.2753, -1.6946, -2.3722],\n",
       "         [ 5.0942, -2.3274, -2.4278],\n",
       "         [ 3.0327, -0.6622, -2.0576],\n",
       "         [ 6.3473, -3.3027, -3.2895],\n",
       "         [ 1.9286, -0.5120, -1.5113],\n",
       "         [ 1.9677, -1.0838, -1.1547],\n",
       "         [ 1.9515, -1.2789, -1.0938],\n",
       "         [ 2.4469, -0.9076, -1.7426],\n",
       "         [ 1.4087, -1.6621, -0.2543],\n",
       "         [ 1.0764, -0.6197, -1.0385],\n",
       "         [ 2.8540, -1.9990, -1.4124],\n",
       "         [ 2.7640, -1.8051, -1.4731],\n",
       "         [ 2.2549, -1.2945, -1.4552],\n",
       "         [ 2.0691, -1.1897, -1.4858],\n",
       "         [ 1.8774, -1.0725, -1.3772],\n",
       "         [ 1.8399, -0.9962, -1.3335],\n",
       "         [ 1.6621, -0.8609, -1.1428],\n",
       "         [ 1.7388, -0.9052, -1.0484],\n",
       "         [ 2.0515, -0.8024, -1.5792],\n",
       "         [ 1.3520, -0.3027, -1.3189],\n",
       "         [ 0.6413,  0.1432, -1.3937],\n",
       "         [ 1.4976, -0.5410, -1.5300],\n",
       "         [ 1.5571, -0.5266, -1.4970],\n",
       "         [ 1.5223, -0.4840, -1.4977],\n",
       "         [ 1.7332, -0.7763, -1.3634],\n",
       "         [ 1.7832, -1.1113, -1.1465],\n",
       "         [ 1.7761, -1.0333, -1.2545],\n",
       "         [ 1.6504, -0.8800, -1.2535],\n",
       "         [ 1.6395, -0.8307, -1.3180],\n",
       "         [ 1.6625, -0.6549, -1.4616],\n",
       "         [ 1.7868, -0.2822, -2.1050],\n",
       "         [ 1.7765, -0.7453, -1.2650],\n",
       "         [ 2.1525, -1.3767, -1.0699],\n",
       "         [ 1.8756, -1.1886, -1.0586],\n",
       "         [ 1.2004,  0.0772, -1.4269],\n",
       "         [ 0.0929,  0.7337, -1.0596],\n",
       "         [ 0.9293, -0.3279, -1.0623],\n",
       "         [ 2.1702, -1.7004, -1.1589],\n",
       "         [ 2.8119, -2.0346, -1.3431],\n",
       "         [ 2.8446, -1.9035, -1.4682],\n",
       "         [ 2.4902, -1.4074, -1.4802],\n",
       "         [ 1.9765, -1.1385, -1.4612],\n",
       "         [ 1.6992, -0.9464, -1.1851],\n",
       "         [ 1.6203, -0.9438, -1.1884],\n",
       "         [ 2.1386, -1.0223, -1.4637],\n",
       "         [ 0.8786,  0.1433, -0.9738],\n",
       "         [ 1.7937, -0.6019, -1.4597],\n",
       "         [ 1.1752, -0.0985, -1.2726],\n",
       "         [ 0.3050,  0.3804, -1.2718],\n",
       "         [ 0.8168, -0.2242, -1.2515],\n",
       "         [ 1.5989, -0.6351, -1.5065],\n",
       "         [ 1.8522, -0.8929, -1.4730],\n",
       "         [ 1.7890, -0.7134, -1.5106],\n",
       "         [ 1.9106, -0.9128, -1.4319],\n",
       "         [ 1.8349, -1.0670, -1.2883],\n",
       "         [ 1.7422, -1.0318, -1.2593],\n",
       "         [ 1.7562, -0.8801, -1.3220],\n",
       "         [ 1.6350, -0.8307, -1.3326],\n",
       "         [ 1.8335, -0.6191, -2.0164],\n",
       "         [ 1.9246, -0.3078, -1.7817],\n",
       "         [ 1.8725, -1.0423, -1.1142],\n",
       "         [ 1.9410, -1.6567, -0.6759],\n",
       "         [ 2.4679, -1.2840, -1.4746],\n",
       "         [ 0.2362,  0.5053, -1.1357],\n",
       "         [ 0.0598,  0.6501, -0.9767],\n",
       "         [ 2.6574, -1.6610, -1.2878],\n",
       "         [ 2.7433, -1.7033, -1.4046],\n",
       "         [ 2.9371, -1.9091, -1.5759],\n",
       "         [ 2.7119, -1.6887, -1.4680],\n",
       "         [ 2.2275, -1.1920, -1.4403],\n",
       "         [ 2.1581, -1.1441, -1.4827],\n",
       "         [ 3.1677, -1.2465, -2.0348],\n",
       "         [ 2.2052, -1.1004, -1.3665],\n",
       "         [ 2.4843, -1.0488, -1.6233],\n",
       "         [ 1.6162, -0.4551, -1.3852],\n",
       "         [ 0.9218,  0.0121, -0.8550],\n",
       "         [ 3.2113, -1.3543, -1.8140],\n",
       "         [ 3.0638, -1.3492, -1.6947],\n",
       "         [ 2.2841, -1.5813, -0.9408],\n",
       "         [ 2.6087, -1.6595, -1.1962],\n",
       "         [ 2.6075, -1.3456, -1.3895],\n",
       "         [ 2.4160, -1.2470, -1.3481],\n",
       "         [ 2.5032, -1.2299, -1.3306],\n",
       "         [ 2.3448, -1.4413, -1.2145],\n",
       "         [ 2.4552, -1.5046, -1.2621],\n",
       "         [ 2.2379, -1.0956, -1.1065],\n",
       "         [ 2.9122, -1.4925, -1.5252],\n",
       "         [ 3.3123, -1.4782, -1.8202],\n",
       "         [ 3.2496, -1.5221, -1.7635],\n",
       "         [ 1.8424, -1.0583, -0.9417],\n",
       "         [ 2.5517, -1.4034, -1.3163],\n",
       "         [ 2.8592, -1.4370, -1.5167],\n",
       "         [ 2.1640, -1.0969, -1.1387],\n",
       "         [ 2.4021, -1.0694, -1.3125],\n",
       "         [ 2.0312, -0.7761, -1.1942],\n",
       "         [ 1.9389, -0.7846, -1.1422]]], device='mps:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc87376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[-1] * 128\n",
    "arr\n",
    "\n",
    "for item in sentiments:\n",
    "    pos=item[\"pos\"]\n",
    "    start = pos[0]\n",
    "    end = pos[0]+pos[1]\n",
    "    arr[start:end] = [item[\"senti\"]] * (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "838e4123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [8, 8], [12, 12], [67, 67], [80, 80], [95, 95], [96, 96]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ba38ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-ASP'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3fdf2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ASP',\n",
       " 'I-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [id2label[label] for label in preds.tolist()]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a91b9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ASP -------- car ------ 1\n",
      "I-ASP -------- quality ------ 1\n",
      "O -------- is ------ -1\n",
      "O -------- very ------ -1\n",
      "O -------- nice ------ -1\n",
      "O -------- but ------ -1\n",
      "O -------- the ------ -1\n",
      "B-ASP -------- controller ------ 0\n",
      "O -------- sucks ------ 0\n",
      "O -------- . ------ 0\n",
      "O -------- the ------ 0\n",
      "B-ASP -------- controller ------ 0\n",
      "O -------- of ------ 0\n",
      "O -------- this ------ 0\n",
      "O -------- car ------ 0\n",
      "O -------- do ------ 0\n",
      "O -------- not ------ 0\n",
      "O -------- works ------ 0\n",
      "O -------- properly ------ 0\n",
      "O -------- and ------ 0\n",
      "O -------- the ------ 0\n",
      "O -------- final ------ 0\n",
      "O -------- in ------ 0\n",
      "O -------- the ------ -1\n",
      "O -------- controller ------ -1\n",
      "O -------- do ------ -1\n",
      "O -------- not ------ -1\n",
      "O -------- rotate ------ -1\n",
      "O -------- fully ------ -1\n",
      "O -------- it ------ -1\n",
      "O -------- only ------ -1\n",
      "O -------- rotate ------ -1\n",
      "O -------- like ------ -1\n",
      "O -------- button ------ -1\n"
     ]
    }
   ],
   "source": [
    "for item in zip(predictions, temp[\"input_ids\"], arr):\n",
    "    word = tokenizer.convert_ids_to_tokens(item[1])\n",
    "    if word in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "        # print(item[0])\n",
    "        continue\n",
    "    print(item[0],\"--------\" ,word, \"------\", item[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c3cc06e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chrome', (2, 2)), ('book', (3, 3))]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_aspects(input_ids, predictions, tokenizer):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    aspects = []\n",
    "    current_aspect = []\n",
    "    current_position = []\n",
    "    current_idx = 0\n",
    "\n",
    "    for idx, (token, label) in enumerate(zip(tokens, predictions[0])):\n",
    "        # Skip special tokens\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            continue\n",
    "            \n",
    "        label_tag = id2label.get(label, \"O\")\n",
    "        \n",
    "        if label_tag == \"B-ASP\":\n",
    "            # Save previous aspect if exists\n",
    "            if current_aspect:\n",
    "                aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "            # Start new aspect\n",
    "            current_aspect = [token]\n",
    "            current_position = [idx]\n",
    "        elif label_tag == \"I-ASP\" and current_aspect:\n",
    "            # Continue current aspect\n",
    "            current_aspect.append(token)\n",
    "            current_position.append(idx)\n",
    "        else:\n",
    "            # End previous aspect if exists\n",
    "            if current_aspect:\n",
    "                aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "                current_aspect = []\n",
    "                current_position = []\n",
    "\n",
    "    # Catch any leftover aspect\n",
    "    if current_aspect:\n",
    "        aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "\n",
    "    # Clean up tokens (remove ##)\n",
    "    clean_aspects = []\n",
    "    for aspect, pos in aspects:\n",
    "        # Handle WordPiece tokens (tokens that start with ##)\n",
    "        cleaned = \"\"\n",
    "        for word in aspect.split():\n",
    "            if word.startswith(\"##\"):\n",
    "                cleaned += word[2:]  # Remove ## prefix\n",
    "            else:\n",
    "                if cleaned:\n",
    "                    cleaned += \" \"\n",
    "                cleaned += word\n",
    "        clean_aspects.append((cleaned, pos))\n",
    "\n",
    "    return clean_aspects\n",
    "\n",
    "\n",
    "extracted_aspects = extract_aspects(input_ids, predictions, tokenizer)\n",
    "extracted_aspects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
