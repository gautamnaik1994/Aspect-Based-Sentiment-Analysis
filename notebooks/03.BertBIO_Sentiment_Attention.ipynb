{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24a975d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0439105",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"O\": 0, \"B-ASP\": 1, \"I-ASP\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "sentiment2id = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1,\n",
    "    \"neutral\": 2\n",
    "}\n",
    "id2sentiment = {v: k for k, v in sentiment2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22cc887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pl.read_parquet(\"../data/processed/df_aspect_pos.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "763c6f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'attention_mask',\n",
       " 'labels',\n",
       " 'aspects_index',\n",
       " 'aspects_sentiment',\n",
       " 'type']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1fdda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "\n",
    "    aspects_index = [item[\"aspects_index\"] for item in batch]          # List[List[List[int]]]\n",
    "    aspects_sentiment = [item[\"aspects_sentiment\"] for item in batch]  # List[List[int]]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"aspects_index\": aspects_index,\n",
    "        \"aspects_sentiment\": aspects_sentiment,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ef7f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aspect_spans(pred_labels):\n",
    "    spans = []\n",
    "    i = 0\n",
    "    while i < len(pred_labels):\n",
    "        if pred_labels[i] == 1:  # B-ASP\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < len(pred_labels) and pred_labels[i] == 2:  # I-ASP\n",
    "                i += 1\n",
    "            end = i - 1\n",
    "            spans.append([start, end])\n",
    "        else:\n",
    "            i += 1\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d3d111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df[\"input_ids\"].to_numpy()\n",
    "        self.attention_mask = df[\"attention_mask\"].to_numpy()\n",
    "        self.labels = df[\"labels\"].to_numpy()\n",
    "        self.aspects_index = df[\"aspects_index\"].to_list()\n",
    "        self.aspects_sentiment = df[\"aspects_sentiment\"].to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            \"aspects_index\": self.aspects_index[idx],           # list of [start, end]\n",
    "            \"aspects_sentiment\": self.aspects_sentiment[idx],   # list of sentiment values\n",
    "        }\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"train\"))\n",
    "val_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"val\"))\n",
    "test_dataset = CustomDataset(df.filter(pl.col(\"type\") == \"test\"))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size , collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d15b8f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "[[1, 1], [2], [2], [0], [1, 1], [1], [0], [1], [1], [1], [0, 0], [1], [1], [0, 2], [1, 1], [0, 0, 0, 0, 0], [0], [0], [1, 1, 1], [1], [1], [0], [1, 0], [1, 1], [1], [1], [1], [0, 1, 1], [1], [1], [0], [1]]\n",
      "[[[9, 9], [11, 11]], [[6, 6]], [[4, 4]], [[2, 2]], [[2, 2], [5, 5]], [[2, 3]], [[2, 3]], [[6, 6]], [[8, 12]], [[2, 2]], [[7, 7], [13, 13]], [[5, 9]], [[2, 3]], [[15, 15], [23, 23]], [[2, 2], [8, 8]], [[6, 6], [9, 9], [13, 13], [19, 20], [30, 30]], [[11, 11]], [[7, 7]], [[3, 3], [6, 6], [12, 12]], [[6, 7]], [[2, 2]], [[13, 20]], [[1, 1], [10, 10]], [[1, 1], [6, 6]], [[11, 12]], [[2, 6]], [[8, 8]], [[32, 32], [8, 10], [17, 17]], [[2, 7]], [[2, 2]], [[2, 2]], [[8, 8]]]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    print(batch[\"attention_mask\"].shape)\n",
    "    print(batch[\"labels\"].shape)\n",
    "    print(batch[\"aspects_sentiment\"])\n",
    "    print(batch[\"aspects_index\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2367963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, 3)  # Sentiment classes: pos, neg, neutral\n",
    "\n",
    "    def forward(self, token_embeddings, aspect_mask):\n",
    "        # token_embeddings: [B, L, H], aspect_mask: [B, L]\n",
    "        aspect_mask = aspect_mask.unsqueeze(-1).expand_as(token_embeddings)  # [B, L, H]\n",
    "        aspect_embeddings = token_embeddings * aspect_mask  # Zero out non-aspect tokens\n",
    "\n",
    "        aspect_pooled = aspect_embeddings.sum(dim=1) / (aspect_mask.sum(dim=1) + 1e-8)  # [B, H]\n",
    "\n",
    "        query = aspect_pooled.unsqueeze(1)  # [B, 1, H]\n",
    "        key = value = token_embeddings  # [B, L, H]\n",
    "\n",
    "        attended_output, attn_weights = self.attention(query, key, value)  # [B, 1, H]\n",
    "        attended_output = self.dropout(attended_output)\n",
    "        attended_output = self.norm(attended_output)\n",
    "\n",
    "        logits = self.classifier(attended_output.squeeze(1))  # [B, 3]\n",
    "        return logits, attn_weights \n",
    "\n",
    "\n",
    "class AspectDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AspectDetectionModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.token_classifier = nn.Linear(self.bert.config.hidden_size, len(label2id))\n",
    "        self.sentiment_classifier = SentimentClassifier(hidden_size=self.bert.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)  # [B, L, H]\n",
    "\n",
    "        token_logits = self.token_classifier(sequence_output)  # For aspect term tagging (BIO)\n",
    "\n",
    "        return token_logits, sequence_output\n",
    "\n",
    "model = AspectDetectionModel().to(\"mps\")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b20ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Aspect Loss: 0.3801, Train Sentiment Loss: 0.4980, Val Aspect Loss: 0.2209, Val Sentiment Loss: 0.4874\n",
      "Epoch [2/10], Train Aspect Loss: 0.1951, Train Sentiment Loss: 0.3486, Val Aspect Loss: 0.1637, Val Sentiment Loss: 0.3560\n",
      "Epoch [3/10], Train Aspect Loss: 0.1467, Train Sentiment Loss: 0.2787, Val Aspect Loss: 0.1377, Val Sentiment Loss: 0.4225\n",
      "Epoch [4/10], Train Aspect Loss: 0.1237, Train Sentiment Loss: 0.2259, Val Aspect Loss: 0.1298, Val Sentiment Loss: 0.3918\n",
      "Epoch [5/10], Train Aspect Loss: 0.1106, Train Sentiment Loss: 0.1848, Val Aspect Loss: 0.1241, Val Sentiment Loss: 0.4247\n",
      "Epoch [6/10], Train Aspect Loss: 0.0997, Train Sentiment Loss: 0.1653, Val Aspect Loss: 0.1194, Val Sentiment Loss: 0.4578\n",
      "Epoch [7/10], Train Aspect Loss: 0.0931, Train Sentiment Loss: 0.1333, Val Aspect Loss: 0.1189, Val Sentiment Loss: 0.4685\n",
      "Epoch [8/10], Train Aspect Loss: 0.0875, Train Sentiment Loss: 0.1155, Val Aspect Loss: 0.1146, Val Sentiment Loss: 0.5045\n",
      "Epoch [9/10], Train Aspect Loss: 0.0832, Train Sentiment Loss: 0.0944, Val Aspect Loss: 0.1153, Val Sentiment Loss: 0.4962\n",
      "Epoch [10/10], Train Aspect Loss: 0.0772, Train Sentiment Loss: 0.0773, Val Aspect Loss: 0.1171, Val Sentiment Loss: 0.5272\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_aspect_train_loss = 0\n",
    "    total_sentiment_train_loss = 0\n",
    "    total_aspect_val_loss = 0\n",
    "    total_sentiment_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"mps\")\n",
    "        attention_mask = batch[\"attention_mask\"].to(\"mps\")\n",
    "        labels = batch[\"labels\"].to(\"mps\")\n",
    "\n",
    "\n",
    "        token_logits, sequence_output = model(input_ids, attention_mask)\n",
    "        aspect_loss = criterion(token_logits.view(-1, len(label2id)), labels.view(-1))\n",
    "        total_aspect_train_loss += aspect_loss.item()\n",
    "\n",
    "\n",
    "      \n",
    "        sentiment_losses = []\n",
    "        for i in range(len(input_ids)):\n",
    "            for aspect_index, sentiment in zip(batch[\"aspects_index\"][i], batch[\"aspects_sentiment\"][i]):\n",
    "                if aspect_index[1] >= sequence_output.size(1):\n",
    "                    continue\n",
    "                # Create a new aspect mask for each aspect instead of modifying in-place\n",
    "                aspect_mask = torch.zeros_like(input_ids, dtype=torch.float).to(\"mps\")\n",
    "                aspect_mask[i, aspect_index[0]:aspect_index[1]+1] = 1\n",
    "                sentiment_logits, _ = model.sentiment_classifier(sequence_output[i].unsqueeze(0), aspect_mask[i].unsqueeze(0))\n",
    "                sentiment_target = torch.tensor([sentiment], dtype=torch.long).to(\"mps\")\n",
    "                sentiment_loss = criterion(sentiment_logits, sentiment_target)\n",
    "                sentiment_losses.append(sentiment_loss)\n",
    "\n",
    "\n",
    "        if sentiment_losses:\n",
    "            sentiment_loss = torch.stack(sentiment_losses).mean()\n",
    "        else:\n",
    "            sentiment_loss = torch.tensor(0.0).to(\"mps\")\n",
    "\n",
    "        total_sentiment_train_loss += sentiment_loss.item()\n",
    "        total_loss = aspect_loss + sentiment_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(\"mps\")\n",
    "            attention_mask = batch[\"attention_mask\"].to(\"mps\")\n",
    "            labels = batch[\"labels\"].to(\"mps\")\n",
    "\n",
    "            token_logits, sequence_output = model(input_ids, attention_mask)\n",
    "            aspect_loss = criterion(token_logits.view(-1, len(label2id)), labels.view(-1))\n",
    "            total_aspect_val_loss += aspect_loss.item()\n",
    "\n",
    "            sentiment_losses = []\n",
    "            preds = torch.argmax(token_logits, dim=2)\n",
    "            for i in range(len(input_ids)):\n",
    "                aspects = extract_aspect_spans(preds[i].cpu().tolist())\n",
    "                for aspect_index, sentiment in zip(batch[\"aspects_index\"][i], batch[\"aspects_sentiment\"][i]):\n",
    "                    if aspect_index in aspects and aspect_index[1] < sequence_output.size(1):\n",
    "                        aspect_mask = torch.zeros_like(input_ids, dtype=torch.float).to(\"mps\")\n",
    "                        aspect_mask[i, aspect_index[0]:aspect_index[1]+1] = 1\n",
    "                        sentiment_logits, _ = model.sentiment_classifier(sequence_output[i].unsqueeze(0), aspect_mask[i].unsqueeze(0))\n",
    "                        sentiment_target = torch.tensor([sentiment], dtype=torch.long).to(\"mps\")\n",
    "                        sentiment_loss = criterion(sentiment_logits.view(-1, 3), sentiment_target)\n",
    "                        sentiment_losses.append(sentiment_loss)\n",
    "\n",
    "            if sentiment_losses:\n",
    "                sentiment_loss = torch.stack(sentiment_losses).mean()\n",
    "            else:\n",
    "                sentiment_loss = torch.tensor(0.0).to(\"mps\")\n",
    "\n",
    "            total_sentiment_val_loss += sentiment_loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "        f\"Train Aspect Loss: {total_aspect_train_loss/len(train_dataloader):.4f}, \"\n",
    "        f\"Train Sentiment Loss: {total_sentiment_train_loss/len(train_dataloader):.4f}, \"\n",
    "        f\"Val Aspect Loss: {total_aspect_val_loss/len(val_dataloader):.4f}, \"\n",
    "        f\"Val Sentiment Loss: {total_sentiment_val_loss/len(val_dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e18f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to ../models/AspectDetectionModel/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "name=\"AspectDetectionModel_Sentiment_Analysis_Attention\"\n",
    "\n",
    "os.makedirs(\"../models/AspectDetectionModel\", exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/AspectDetectionModel/\" + name + \".pth\")\n",
    "\n",
    "model_config = {\n",
    "\t\"hidden_size\": model.bert.config.hidden_size,\n",
    "\t\"num_labels\": len(label2id),\n",
    "\t\"id2label\": id2label,\n",
    "\t\"label2id\": label2id,\n",
    "\t\"name\": name\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"../models/AspectDetectionModel/{name}_config.json\", \"w\") as f:\n",
    "\tjson.dump(model_config, f)\n",
    "\n",
    "print(\"Model saved successfully to ../models/AspectDetectionModel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c62198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AspectDetectionModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (token_classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (sentiment_classifier): SentimentClassifier(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name=\"AspectDetectionModel_Sentiment_Analysis_Attention\"\n",
    "model = AspectDetectionModel()\n",
    "model.load_state_dict(torch.load(\"../models/AspectDetectionModel/\" + name + \".pth\"))\n",
    "model = model.to(\"mps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee4af7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2482, 3737, 2003, 2200, 3835, 2021, 1996, 11486, 19237, 1012, 1996, 11486, 1997, 2023, 2482, 2079, 2025, 2573, 7919, 1998, 1996, 2345, 1999, 1996, 11486, 2079, 2025, 24357, 3929, 2009, 2069, 24357, 2066, 6462, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "temp = tokenizer((\"\"\"Car quality is very nice but the controller sucks . The controller of this car do not works properly and the final in the controller do not rotate fully it only rotate like button\"\"\").split(), is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f985937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 6556, 2491, 2482, 2003, 4569, 1010, 3435, 1010, 1998, 3733, 2000, 5047, 1517, 3819, 2005, 4268, 999, 1996, 3857, 3737, 2003, 23073, 1998, 2009, 3216, 15299, 2006, 2367, 9972, 1012, 6046, 2166, 2003, 11519, 1998, 1996, 7711, 2024, 2200, 26651, 1012, 1037, 2307, 5592, 2005, 4268, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "temp = tokenizer((\"\"\"This remote control car is fun, fast, and easy to handle—perfect for kids! The build quality is sturdy and it runs smoothly on different surfaces. Battery life is decent and the controls are very responsive. A great gift for kids!\"\"\").split(), is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a232f2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2152, 7597, 2017, 1005, 2128, 2183, 2000, 3477, 2003, 2005, 1996, 3193, 2025, 2005, 1996, 2833, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "temp = tokenizer((\"\"\"the high prices you ' re going to pay is for the view not for the food .\"\"\").split(), is_split_into_words=True,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=128)\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28521159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], device='mps:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"mps\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    \n",
    "    input_ids = torch.tensor(temp[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(temp[\"attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    logits, sequence_output = model(input_ids, attention_mask)\n",
    "    # decode the logits to get the predicted labels\n",
    "    preds = torch.argmax(logits, dim=2)[0]\n",
    "    aspects = extract_aspect_spans(preds.cpu().tolist())\n",
    "    sentiments=[]\n",
    "    for aspect in aspects:\n",
    "        aspect_mask = torch.zeros_like(input_ids, dtype=torch.long).to(\"mps\")\n",
    "        aspect_mask[0, aspect[0]:aspect[1]+1] = 1\n",
    "        # print(\"am\", aspect_mask[0].unsqueeze(0))\n",
    "        sentiment_logits, _ = model.sentiment_classifier(\n",
    "            sequence_output, aspect_mask\n",
    "        )\n",
    "        sentiments.append({\"pos\":aspect,\"senti\":torch.argmax(sentiment_logits, dim=1).item()})\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "000e9f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.4958, -1.5557, -3.3050],\n",
       "         [ 5.4345, -3.3632, -3.1920],\n",
       "         [ 5.1599, -2.9615, -3.2711],\n",
       "         [ 0.8643,  2.0685, -3.0147],\n",
       "         [ 5.2092, -3.1613, -2.9502],\n",
       "         [ 5.2654, -3.5524, -2.3321],\n",
       "         [ 5.3565, -3.4839, -2.6964],\n",
       "         [ 5.1150, -2.9785, -2.4061],\n",
       "         [ 5.0020, -3.0589, -2.7768],\n",
       "         [ 4.6132, -2.4140, -3.0822],\n",
       "         [ 5.2563, -3.0737, -3.2871],\n",
       "         [ 4.7397, -2.7990, -3.0575],\n",
       "         [ 4.3096, -2.6864, -2.9525],\n",
       "         [-0.3268,  2.3367, -2.5131],\n",
       "         [ 5.2072, -2.7667, -3.4396],\n",
       "         [ 5.1848, -2.9837, -3.0111],\n",
       "         [ 4.4994, -2.8495, -3.1791],\n",
       "         [-0.4544,  3.5515, -3.1453],\n",
       "         [ 5.8347, -2.7945, -3.2315],\n",
       "         [ 5.8347, -2.8297, -3.2292],\n",
       "         [ 2.1516, -0.9112, -1.5927],\n",
       "         [ 2.6923, -1.1390, -1.8864],\n",
       "         [ 1.6735, -0.6768, -1.4028],\n",
       "         [ 3.0229, -1.3560, -2.0624],\n",
       "         [ 2.8599, -1.2942, -1.9649],\n",
       "         [ 2.1354, -1.0662, -1.8766],\n",
       "         [ 2.0008, -1.0815, -1.8965],\n",
       "         [ 1.5878, -0.6881, -1.4563],\n",
       "         [ 2.1325, -0.9559, -1.7656],\n",
       "         [ 2.5704, -1.0623, -2.0035],\n",
       "         [ 1.4956, -0.1802, -1.3157],\n",
       "         [ 1.8466, -0.3493, -1.5448],\n",
       "         [ 2.4477, -0.6350, -1.8545],\n",
       "         [ 1.8653, -0.3557, -1.4424],\n",
       "         [ 1.8824, -0.6084, -1.7136],\n",
       "         [ 1.5348, -0.0659, -1.2855],\n",
       "         [ 0.8112,  0.0105, -1.0349],\n",
       "         [ 1.0651, -0.2591, -1.0428],\n",
       "         [ 0.8676, -0.0451, -0.9600],\n",
       "         [ 0.6907, -0.0753, -1.1107],\n",
       "         [ 0.5440, -0.0414, -0.9798],\n",
       "         [ 0.4268,  0.0379, -0.7712],\n",
       "         [ 0.4681,  0.0290, -1.2317],\n",
       "         [ 1.0955, -0.2016, -1.5987],\n",
       "         [ 0.8635, -0.1138, -1.3866],\n",
       "         [ 2.3144, -0.7431, -2.0796],\n",
       "         [ 0.9573, -0.0697, -1.5045],\n",
       "         [ 0.8153, -0.0211, -1.3257],\n",
       "         [ 2.9468, -1.1394, -2.1276],\n",
       "         [ 2.2312, -0.9079, -1.6059],\n",
       "         [ 2.2700, -0.9574, -1.6978],\n",
       "         [ 2.2688, -0.9799, -1.8205],\n",
       "         [ 2.0807, -0.9246, -1.7769],\n",
       "         [ 2.5180, -1.2490, -2.0074],\n",
       "         [ 1.9468, -0.9439, -1.7080],\n",
       "         [ 1.7355, -0.9047, -1.7299],\n",
       "         [ 1.0502, -0.3807, -1.0997],\n",
       "         [ 0.6538, -0.0185, -0.8318],\n",
       "         [ 0.5988,  0.2562, -0.8488],\n",
       "         [ 2.3317, -0.5626, -1.7060],\n",
       "         [ 3.1711, -1.4582, -1.1746],\n",
       "         [ 2.8411, -1.1609, -1.4467],\n",
       "         [ 1.8186, -0.4364, -1.2289],\n",
       "         [ 0.8997,  0.0485, -1.1536],\n",
       "         [ 1.0986, -0.1181, -1.0163],\n",
       "         [ 0.9993, -0.0682, -1.0220],\n",
       "         [ 0.9861, -0.2346, -0.9682],\n",
       "         [ 0.8041, -0.2741, -1.1508],\n",
       "         [ 0.3925,  0.0872, -0.9601],\n",
       "         [ 0.5403, -0.5154, -0.6737],\n",
       "         [ 1.1599, -0.4917, -1.2444],\n",
       "         [ 0.9308, -0.1558, -1.2734],\n",
       "         [ 1.2341, -0.3055, -1.6504],\n",
       "         [ 2.5963, -0.9289, -2.0618],\n",
       "         [ 2.5202, -1.0103, -1.8381],\n",
       "         [ 3.0054, -1.2193, -2.0636],\n",
       "         [ 2.5744, -0.9876, -1.8304],\n",
       "         [ 2.8024, -1.1678, -1.9143],\n",
       "         [ 3.3478, -1.4249, -2.2306],\n",
       "         [ 2.2576, -0.9159, -1.6993],\n",
       "         [ 2.3887, -1.0146, -1.8144],\n",
       "         [ 2.5892, -1.1672, -1.9728],\n",
       "         [ 2.3592, -1.0690, -1.8940],\n",
       "         [ 2.8957, -1.2460, -2.1427],\n",
       "         [ 2.8196, -1.2928, -2.1213],\n",
       "         [ 1.9876, -0.9707, -1.8241],\n",
       "         [ 2.4798, -1.0337, -1.9289],\n",
       "         [ 2.3068, -0.6509, -1.9048],\n",
       "         [ 2.5202, -0.7622, -2.0858],\n",
       "         [ 2.8815, -0.8309, -2.1564],\n",
       "         [ 3.3342, -1.3910, -1.5774],\n",
       "         [ 1.4234, -0.3770, -1.5225],\n",
       "         [ 1.7962, -0.6046, -1.6782],\n",
       "         [ 1.0119,  0.0188, -1.0618],\n",
       "         [ 0.8282,  0.1010, -1.0259],\n",
       "         [ 1.0278, -0.3116, -1.0424],\n",
       "         [ 0.6962, -0.0991, -1.0324],\n",
       "         [ 1.5566, -0.4371, -1.8303],\n",
       "         [ 0.8792, -0.3694, -1.3410],\n",
       "         [ 0.8849, -0.1385, -1.4262],\n",
       "         [ 1.4263, -0.4481, -1.5504],\n",
       "         [ 2.0579, -0.6078, -2.0433],\n",
       "         [ 1.1313, -0.2178, -1.5271],\n",
       "         [ 3.4555, -1.5181, -2.3571],\n",
       "         [ 3.4337, -1.4974, -2.2772],\n",
       "         [ 2.0466, -0.7607, -1.4863],\n",
       "         [ 3.4068, -1.4774, -2.2415],\n",
       "         [ 3.2212, -1.3586, -2.0905],\n",
       "         [ 3.2559, -1.3917, -2.1473],\n",
       "         [ 3.1166, -1.2873, -2.0487],\n",
       "         [ 2.1985, -0.9514, -1.7920],\n",
       "         [ 2.6461, -1.2069, -2.0212],\n",
       "         [ 2.7082, -1.1301, -2.0121],\n",
       "         [ 2.4234, -0.9178, -1.8882],\n",
       "         [ 2.8000, -1.0707, -2.0926],\n",
       "         [ 2.4524, -0.7265, -1.9273],\n",
       "         [ 2.2888, -0.7431, -1.8543],\n",
       "         [ 2.8119, -0.9259, -2.1792],\n",
       "         [ 2.3705, -0.6366, -1.9813],\n",
       "         [ 2.2118, -0.6948, -1.9036],\n",
       "         [ 2.0242, -0.6566, -1.7553],\n",
       "         [ 1.5069, -0.4281, -1.4535],\n",
       "         [ 1.6270, -0.5104, -1.5982],\n",
       "         [ 2.1610, -0.7010, -1.9543],\n",
       "         [ 1.2998, -0.3551, -1.4278],\n",
       "         [ 1.5164, -0.5062, -1.6573],\n",
       "         [ 1.3409, -0.3717, -1.6469],\n",
       "         [ 0.6137,  0.2219, -1.4864]]], device='mps:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc87376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[-1] * 128\n",
    "arr\n",
    "\n",
    "for item in sentiments:\n",
    "    pos=item[\"pos\"]\n",
    "    start = pos[0]\n",
    "    end = pos[0]+pos[1]\n",
    "    arr[start:end] = [item[\"senti\"]] * (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "838e4123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 3], [13, 13], [17, 17]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3fdf2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ASP',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [id2label[label] for label in preds.tolist()]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a91b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BIO Tag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3f33d168-0cb4-4691-b425-db1640a8538f",
       "rows": [
        [
         "0",
         "O",
         "the",
         ""
        ],
        [
         "1",
         "O",
         "high",
         ""
        ],
        [
         "2",
         "B-ASP",
         "prices",
         "negative"
        ],
        [
         "3",
         "O",
         "you",
         ""
        ],
        [
         "4",
         "O",
         "'",
         ""
        ],
        [
         "5",
         "O",
         "re",
         ""
        ],
        [
         "6",
         "O",
         "going",
         ""
        ],
        [
         "7",
         "O",
         "to",
         ""
        ],
        [
         "8",
         "O",
         "pay",
         ""
        ],
        [
         "9",
         "O",
         "is",
         ""
        ],
        [
         "10",
         "O",
         "for",
         ""
        ],
        [
         "11",
         "O",
         "the",
         ""
        ],
        [
         "12",
         "B-ASP",
         "view",
         "negative"
        ],
        [
         "13",
         "O",
         "not",
         ""
        ],
        [
         "14",
         "O",
         "for",
         ""
        ],
        [
         "15",
         "O",
         "the",
         ""
        ],
        [
         "16",
         "B-ASP",
         "food",
         "negative"
        ],
        [
         "17",
         "O",
         ".",
         ""
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 18
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BIO Tag</th>\n",
       "      <th>Word</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>high</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-ASP</td>\n",
       "      <td>prices</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>you</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O</td>\n",
       "      <td>re</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O</td>\n",
       "      <td>going</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O</td>\n",
       "      <td>pay</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O</td>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B-ASP</td>\n",
       "      <td>view</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>O</td>\n",
       "      <td>not</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>O</td>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B-ASP</td>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>O</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BIO Tag    Word Sentiment\n",
       "0        O     the          \n",
       "1        O    high          \n",
       "2    B-ASP  prices  negative\n",
       "3        O     you          \n",
       "4        O       '          \n",
       "5        O      re          \n",
       "6        O   going          \n",
       "7        O      to          \n",
       "8        O     pay          \n",
       "9        O      is          \n",
       "10       O     for          \n",
       "11       O     the          \n",
       "12   B-ASP    view  negative\n",
       "13       O     not          \n",
       "14       O     for          \n",
       "15       O     the          \n",
       "16   B-ASP    food  negative\n",
       "17       O       .          "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_sentiment(bio_tag, sentiment):\n",
    "    if(bio_tag in [\"B-ASP\", \"I-ASP\"]):\n",
    "        return id2sentiment[sentiment]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for item in zip(predictions, temp[\"input_ids\"], arr):\n",
    "    word = tokenizer.convert_ids_to_tokens(item[1])\n",
    "    if word in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "        continue\n",
    "    sentiment = print_sentiment(item[0], item[2])\n",
    "    rows.append({\n",
    "        \"BIO Tag\": item[0],\n",
    "        \"Word\": word,\n",
    "        \"Sentiment\": sentiment\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02d3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_html(index=False).replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc06e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chrome', (2, 2)), ('book', (3, 3))]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_aspects(input_ids, predictions, tokenizer):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    aspects = []\n",
    "    current_aspect = []\n",
    "    current_position = []\n",
    "    current_idx = 0\n",
    "\n",
    "    for idx, (token, label) in enumerate(zip(tokens, predictions[0])):\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            continue\n",
    "            \n",
    "        label_tag = id2label.get(label, \"O\")\n",
    "        \n",
    "        if label_tag == \"B-ASP\":\n",
    "            if current_aspect:\n",
    "                aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "            current_aspect = [token]\n",
    "            current_position = [idx]\n",
    "        elif label_tag == \"I-ASP\" and current_aspect:\n",
    "            current_aspect.append(token)\n",
    "            current_position.append(idx)\n",
    "        else:\n",
    "            if current_aspect:\n",
    "                aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "                current_aspect = []\n",
    "                current_position = []\n",
    "\n",
    "    if current_aspect:\n",
    "        aspects.append((\" \".join(current_aspect), (current_position[0], current_position[-1])))\n",
    "\n",
    "    clean_aspects = []\n",
    "    for aspect, pos in aspects:\n",
    "        cleaned = \"\"\n",
    "        for word in aspect.split():\n",
    "            if word.startswith(\"##\"):\n",
    "                cleaned += word[2:]  # Remove ## prefix\n",
    "            else:\n",
    "                if cleaned:\n",
    "                    cleaned += \" \"\n",
    "                cleaned += word\n",
    "        clean_aspects.append((cleaned, pos))\n",
    "\n",
    "    return clean_aspects\n",
    "\n",
    "\n",
    "extracted_aspects = extract_aspects(input_ids, predictions, tokenizer)\n",
    "extracted_aspects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
