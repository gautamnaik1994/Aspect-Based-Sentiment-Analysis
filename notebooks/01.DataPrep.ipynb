{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a975d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b5c7c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines=[]\n",
    "test_lines=[]\n",
    "val_lines=[]\n",
    "\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/501.Laptop14/laptop_quad_train.tsv.jsonl\", \"r\") as f:\n",
    "    train_lines = f.readlines()\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/501.Laptop14/laptop_quad_test.tsv.jsonl\", \"r\") as f:\n",
    "    test_lines = f.readlines()\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/501.Laptop14/laptop_quad_dev.tsv.jsonl\", \"r\") as f:\n",
    "    val_lines = f.readlines()\n",
    "\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/502.Restaurant14/train.jsonl\", \"r\") as f:\n",
    "    train_lines += f.readlines()\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/502.Restaurant14/test.jsonl\", \"r\") as f:\n",
    "    test_lines += f.readlines()\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/502.Restaurant14/dev.jsonl\", \"r\") as f:\n",
    "    val_lines += f.readlines()\n",
    "\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/503.Restaurant15/train.jsonl\", \"r\") as f:\n",
    "    train_lines += f.readlines()\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/503.Restaurant15/test.jsonl\", \"r\") as f:\n",
    "    test_lines += f.readlines()\n",
    "    \n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/503.Restaurant15/dev.jsonl\", \"r\") as f:\n",
    "    val_lines += f.readlines()\n",
    "\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/504.Restaurant16/rest16_quad_train.tsv.jsonl\", \"r\") as f:\n",
    "    train_lines += f.readlines()\n",
    "\n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/504.Restaurant16/rest16_quad_test.tsv.jsonl\", \"r\") as f:\n",
    "    test_lines += f.readlines()\n",
    "    \n",
    "with open(\"../data/raw/absa_datasets/acos_datasets/504.Restaurant16/rest16_quad_dev.tsv.jsonl\", \"r\") as f:\n",
    "    val_lines += f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "910ab97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(lines):\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line_obj = json.loads(line)\n",
    "        valid_line = True\n",
    "        for label in line_obj[\"labels\"]:\n",
    "            if label[\"aspect\"] == \"NULL\" or label[\"polarity\"] == \"NULL\":\n",
    "                valid_line = False\n",
    "                break\n",
    "        if valid_line and line_obj[\"labels\"]:  # Only append if it has valid labels\n",
    "            cleaned_lines.append(line_obj)\n",
    "    return cleaned_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d9ee249",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = clean_data(train_lines)\n",
    "test_lines = clean_data(test_lines)\n",
    "val_lines = clean_data(val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff6cd3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"now i ' m really bummed that i have a very nice looking chromebook with a beautiful screen that is totally unusable .\",\n",
       " 'labels': [{'aspect': 'chromebook',\n",
       "   'opinion': 'nice',\n",
       "   'polarity': 'positive',\n",
       "   'category': 'LAPTOP#DESIGN_FEATURES'},\n",
       "  {'aspect': 'chromebook',\n",
       "   'opinion': 'bummed',\n",
       "   'polarity': 'negative',\n",
       "   'category': 'LAPTOP#OPERATION_PERFORMANCE'},\n",
       "  {'aspect': 'chromebook',\n",
       "   'opinion': 'unusable',\n",
       "   'polarity': 'negative',\n",
       "   'category': 'LAPTOP#OPERATION_PERFORMANCE'},\n",
       "  {'aspect': 'screen',\n",
       "   'opinion': 'beautiful',\n",
       "   'polarity': 'positive',\n",
       "   'category': 'DISPLAY#OPERATION_PERFORMANCE'}]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c33886b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5050, 1827, 828)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines), len(test_lines), len(val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a24494",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"O\": 0, \"B-ASP\": 1, \"I-ASP\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "sentiment2id = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1,\n",
    "    \"neutral\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'now', 'i', \"'\", 'm', 'really', 'bum', '##med', 'that', 'i', 'have', 'a', 'very', 'nice', 'looking', 'chrome', '##book', 'with', 'a', 'beautiful', 'screen', 'that', 'is', 'totally', 'un', '##usa', '##ble', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Input IDs: [101, 2085, 1045, 1005, 1049, 2428, 26352, 7583, 2008, 1045, 2031, 1037, 2200, 3835, 2559, 18546, 8654, 2007, 1037, 3376, 3898, 2008, 2003, 6135, 4895, 10383, 3468, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Aspect Spans: [[15, 16], [20, 20]]\n",
      "Polarities: [1, 1]\n",
      "Token Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'O', 'O', 'O', 'B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'text': \"now i ' m really bummed that i have a very nice looking chromebook with a beautiful screen that is totally unusable .\",\n",
    "    'labels': [\n",
    "        {'aspect': 'chromebook', 'opinion': 'nice', 'polarity': 'positive', 'category': 'LAPTOP#DESIGN_FEATURES'},\n",
    "        {'aspect': 'chromebook', 'opinion': 'bummed', 'polarity': 'negative', 'category': 'LAPTOP#OPERATION_PERFORMANCE'},\n",
    "        {'aspect': 'chromebook', 'opinion': 'unusable', 'polarity': 'negative', 'category': 'LAPTOP#OPERATION_PERFORMANCE'},\n",
    "        {'aspect': 'screen', 'opinion': 'beautiful', 'polarity': 'positive', 'category': 'DISPLAY#OPERATION_PERFORMANCE'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "include_opinions = False \n",
    "\n",
    "encoding = tokenizer(\n",
    "    data['text'],\n",
    "    return_offsets_mapping=True,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=128,\n",
    ")\n",
    "\n",
    "offset_mapping = encoding['offset_mapping']\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "token_labels = [label2id['O']] * len(input_ids)\n",
    "\n",
    "aspect_spans = []\n",
    "polarities = []\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "def find_all_spans(text, word):\n",
    "    spans = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = text.lower().find(word.lower(), start)\n",
    "        if start == -1:\n",
    "            break\n",
    "        end = start + len(word)\n",
    "        spans.append((start, end))\n",
    "        start = end\n",
    "    return spans\n",
    "\n",
    "seen_aspects = set()\n",
    "\n",
    "for label_entry in data['labels']:\n",
    "    aspect_word = label_entry['aspect']\n",
    "    opinion_word = label_entry['opinion']\n",
    "    polarity = label_entry['polarity']\n",
    "    \n",
    "    if include_opinions:\n",
    "        key = (aspect_word.lower(), opinion_word.lower())\n",
    "    else:\n",
    "        key = aspect_word.lower()\n",
    "    \n",
    "    if key in seen_aspects:\n",
    "        continue  \n",
    "    seen_aspects.add(key)\n",
    "    \n",
    "    asp_spans = find_all_spans(data['text'], aspect_word)\n",
    "    \n",
    "    for asp_start, asp_end in asp_spans:\n",
    "        start_token_idx, end_token_idx = None, None\n",
    "\n",
    "        for idx, (tok_start, tok_end) in enumerate(offset_mapping):\n",
    "            if tok_start == tok_end:\n",
    "                continue\n",
    "            if start_token_idx is None and (tok_start >= asp_start and tok_end <= asp_end):\n",
    "                start_token_idx = idx\n",
    "            if start_token_idx is not None:\n",
    "                if tok_start >= asp_start and tok_end <= asp_end:\n",
    "                    end_token_idx = idx\n",
    "        \n",
    "        if start_token_idx is not None and end_token_idx is not None:\n",
    "            aspect_spans.append([start_token_idx, end_token_idx])\n",
    "            polarities.append(polarity)\n",
    "\n",
    "            token_labels[start_token_idx] = label2id['B-ASP']\n",
    "            for i in range(start_token_idx + 1, end_token_idx + 1):\n",
    "                token_labels[i] = label2id['I-ASP']\n",
    "\n",
    "\n",
    "token_labels[0] = -100\n",
    "token_labels = [token_labels[i] if attention_mask[i] == 1 else -100 for i in range(len(token_labels))]\n",
    "\n",
    "\n",
    "polarities = [sentiment2id[polarity] for polarity in polarities]\n",
    "\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "print(\"Aspect Spans:\", aspect_spans)\n",
    "print(\"Polarities:\", polarities)\n",
    "print(\"Token Labels:\", [id2label[label] for label in token_labels if label != -100])\n",
    "\n",
    "output = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "    'token_labels': token_labels,\n",
    "    'aspect_spans': aspect_spans,\n",
    "    'polarities': polarities,\n",
    "    'original_text': data['text']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0439105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_v2(data):\n",
    "    \n",
    "    include_opinions = False \n",
    "    encoding = tokenizer(\n",
    "        data['text'],\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    offset_mapping = encoding['offset_mapping']\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    \n",
    "    token_labels = [label2id['O']] * len(input_ids)\n",
    "\n",
    "    \n",
    "    aspect_spans = []\n",
    "    polarities = []\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    def find_all_spans(text, word):\n",
    "        spans = []\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = text.lower().find(word.lower(), start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            end = start + len(word)\n",
    "            spans.append((start, end))\n",
    "            start = end\n",
    "        return spans\n",
    "\n",
    "    seen_aspects = set()\n",
    "\n",
    "    for label_entry in data['labels']:\n",
    "        aspect_word = label_entry['aspect']\n",
    "        opinion_word = label_entry['opinion']\n",
    "        polarity = label_entry['polarity']\n",
    "        \n",
    "        if include_opinions:\n",
    "            key = (aspect_word.lower(), opinion_word.lower())\n",
    "        else:\n",
    "            key = aspect_word.lower()\n",
    "        \n",
    "        if key in seen_aspects:\n",
    "            continue \n",
    "        seen_aspects.add(key)\n",
    "        \n",
    "        asp_spans = find_all_spans(data['text'], aspect_word)\n",
    "        \n",
    "        for asp_start, asp_end in asp_spans:\n",
    "            start_token_idx, end_token_idx = None, None\n",
    "\n",
    "            for idx, (tok_start, tok_end) in enumerate(offset_mapping):\n",
    "                if tok_start == tok_end:\n",
    "                    continue\n",
    "                if start_token_idx is None and (tok_start >= asp_start and tok_end <= asp_end):\n",
    "                    start_token_idx = idx\n",
    "                if start_token_idx is not None:\n",
    "                    if tok_start >= asp_start and tok_end <= asp_end:\n",
    "                        end_token_idx = idx\n",
    "            \n",
    "            if start_token_idx is not None and end_token_idx is not None:\n",
    "                aspect_spans.append([start_token_idx, end_token_idx])\n",
    "                polarities.append(polarity)\n",
    "\n",
    "                # Also label tokens for extraction task\n",
    "                token_labels[start_token_idx] = label2id['B-ASP']\n",
    "                for i in range(start_token_idx + 1, end_token_idx + 1):\n",
    "                    token_labels[i] = label2id['I-ASP']\n",
    "\n",
    "\n",
    "    token_labels[0] = -100\n",
    "    token_labels = [token_labels[i] if attention_mask[i] == 1 else -100 for i in range(len(token_labels))]\n",
    "\n",
    "\n",
    "    polarities = [sentiment2id[polarity] for polarity in polarities]\n",
    "    output = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': token_labels,\n",
    "        'aspects_index': aspect_spans,\n",
    "        'aspects_sentiment': polarities,\n",
    "        # 'original_text': data['text']\n",
    "    }\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "421342eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame()\n",
    "for line in train_lines:\n",
    "    tokenized = tokenize_and_align_v2(line)\n",
    "    df = df.vstack(pl.DataFrame({\n",
    "        # \"text\": [tokenized[\"text\"]],\n",
    "        \"input_ids\": [tokenized[\"input_ids\"]],\n",
    "        \"attention_mask\": [tokenized[\"attention_mask\"]],\n",
    "        \"labels\": [tokenized[\"labels\"]],\n",
    "        \"aspects_index\": [tokenized[\"aspects_index\"]],\n",
    "        \"aspects_sentiment\": [tokenized[\"aspects_sentiment\"]],\n",
    "        \"type\": \"train\"\n",
    "    }))\n",
    "for line in test_lines:\n",
    "    tokenized = tokenize_and_align_v2(line)\n",
    "    df = df.vstack(pl.DataFrame({\n",
    "        \"input_ids\": [tokenized[\"input_ids\"]],\n",
    "        \"attention_mask\": [tokenized[\"attention_mask\"]],\n",
    "        \"labels\": [tokenized[\"labels\"]],\n",
    "        \"aspects_index\": [tokenized[\"aspects_index\"]],\n",
    "        \"aspects_sentiment\": [tokenized[\"aspects_sentiment\"]],\n",
    "        \"type\": \"test\"\n",
    "    }))\n",
    "for line in val_lines:\n",
    "    tokenized = tokenize_and_align_v2(line)\n",
    "    df = df.vstack(pl.DataFrame({\n",
    "        \"input_ids\": [tokenized[\"input_ids\"]],\n",
    "        \"attention_mask\": [tokenized[\"attention_mask\"]],\n",
    "        \"labels\": [tokenized[\"labels\"]],\n",
    "        \"aspects_index\": [tokenized[\"aspects_index\"]],\n",
    "        \"aspects_sentiment\": [tokenized[\"aspects_sentiment\"]],\n",
    "        \"type\": \"val\"\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22cc887e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>input_ids</th><th>attention_mask</th><th>labels</th><th>aspects_index</th><th>aspects_sentiment</th><th>type</th></tr><tr><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[list[i64]]</td><td>list[i64]</td><td>str</td></tr></thead><tbody><tr><td>[101, 2043, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[9, 11], [3, 3]]</td><td>[0, 0]</td><td>&quot;test&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[2, 2], [7, 7]]</td><td>[1, 1]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[2, 2], [20, 25]]</td><td>[0, 2]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[2, 3]]</td><td>[1]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 5341, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 1, … -100]</td><td>[[1, 2]]</td><td>[1]</td><td>&quot;val&quot;</td></tr><tr><td>[101, 2096, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[7, 7]]</td><td>[1]</td><td>&quot;test&quot;</td></tr><tr><td>[101, 2023, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[17, 22]]</td><td>[0]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 5983, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[5, 5]]</td><td>[1]</td><td>&quot;val&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[2, 2], [7, 7]]</td><td>[1, 1]</td><td>&quot;test&quot;</td></tr><tr><td>[101, 2049, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[4, 4], [32, 32], [37, 37]]</td><td>[0, 0, 1]</td><td>&quot;train&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 6)\n",
       "┌───────────────────┬────────────────┬─────────────┬───────────────────┬───────────────────┬───────┐\n",
       "│ input_ids         ┆ attention_mask ┆ labels      ┆ aspects_index     ┆ aspects_sentiment ┆ type  │\n",
       "│ ---               ┆ ---            ┆ ---         ┆ ---               ┆ ---               ┆ ---   │\n",
       "│ list[i64]         ┆ list[i64]      ┆ list[i64]   ┆ list[list[i64]]   ┆ list[i64]         ┆ str   │\n",
       "╞═══════════════════╪════════════════╪═════════════╪═══════════════════╪═══════════════════╪═══════╡\n",
       "│ [101, 2043, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[9, 11], [3, 3]] ┆ [0, 0]            ┆ test  │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[2, 2], [7, 7]]  ┆ [1, 1]            ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[2, 2], [20,     ┆ [0, 2]            ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆ 25]]              ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[2, 3]]          ┆ [1]               ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 5341, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 1, … ┆ [[1, 2]]          ┆ [1]               ┆ val   │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 2096, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[7, 7]]          ┆ [1]               ┆ test  │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 2023, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[17, 22]]        ┆ [0]               ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 5983, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[5, 5]]          ┆ [1]               ┆ val   │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[2, 2], [7, 7]]  ┆ [1, 1]            ┆ test  │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 2049, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[4, 4], [32,     ┆ [0, 0, 1]         ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆ 32], [37, 37]]    ┆                   ┆       │\n",
       "└───────────────────┴────────────────┴─────────────┴───────────────────┴───────────────────┴───────┘"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "db44f2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1827"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.filter(pl.col(\"type\")== 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5102d8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7_705, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>input_ids</th><th>attention_mask</th><th>labels</th><th>aspects_index</th><th>aspects_sentiment</th><th>type</th></tr><tr><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[list[i64]]</td><td>list[i64]</td><td>str</td></tr></thead><tbody><tr><td>[101, 9078, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 1, … -100]</td><td>[[1, 2]]</td><td>[2]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[19, 20]]</td><td>[0]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[2, 2]]</td><td>[0]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 2255, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[13, 14]]</td><td>[0]</td><td>&quot;train&quot;</td></tr><tr><td>[101, 2823, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[21, 21]]</td><td>[0]</td><td>&quot;train&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>[101, 1045, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[4, 5]]</td><td>[1]</td><td>&quot;val&quot;</td></tr><tr><td>[101, 2079, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[8, 8]]</td><td>[0]</td><td>&quot;val&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[2, 2], [7, 7]]</td><td>[0, 0]</td><td>&quot;val&quot;</td></tr><tr><td>[101, 9467, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[9, 9], [14, 15]]</td><td>[0, 0]</td><td>&quot;val&quot;</td></tr><tr><td>[101, 1996, … 0]</td><td>[1, 1, … 0]</td><td>[-100, 0, … -100]</td><td>[[2, 2]]</td><td>[1]</td><td>&quot;val&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7_705, 6)\n",
       "┌───────────────────┬────────────────┬─────────────┬───────────────────┬───────────────────┬───────┐\n",
       "│ input_ids         ┆ attention_mask ┆ labels      ┆ aspects_index     ┆ aspects_sentiment ┆ type  │\n",
       "│ ---               ┆ ---            ┆ ---         ┆ ---               ┆ ---               ┆ ---   │\n",
       "│ list[i64]         ┆ list[i64]      ┆ list[i64]   ┆ list[list[i64]]   ┆ list[i64]         ┆ str   │\n",
       "╞═══════════════════╪════════════════╪═════════════╪═══════════════════╪═══════════════════╪═══════╡\n",
       "│ [101, 9078, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 1, … ┆ [[1, 2]]          ┆ [2]               ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[19, 20]]        ┆ [0]               ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[2, 2]]          ┆ [0]               ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 2255, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[13, 14]]        ┆ [0]               ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 2823, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[21, 21]]        ┆ [0]               ┆ train │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ …                 ┆ …              ┆ …           ┆ …                 ┆ …                 ┆ …     │\n",
       "│ [101, 1045, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[4, 5]]          ┆ [1]               ┆ val   │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 2079, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[8, 8]]          ┆ [0]               ┆ val   │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[2, 2], [7, 7]]  ┆ [0, 0]            ┆ val   │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "│ [101, 9467, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[9, 9], [14,     ┆ [0, 0]            ┆ val   │\n",
       "│                   ┆                ┆ -100]       ┆ 15]]              ┆                   ┆       │\n",
       "│ [101, 1996, … 0]  ┆ [1, 1, … 0]    ┆ [-100, 0, … ┆ [[2, 2]]          ┆ [1]               ┆ val   │\n",
       "│                   ┆                ┆ -100]       ┆                   ┆                   ┆       │\n",
       "└───────────────────┴────────────────┴─────────────┴───────────────────┴───────────────────┴───────┘"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=df.filter(pl.col(\"aspects_index\").list.len() > 0)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "997a4fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7705"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f636ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.write_parquet(\"../data/processed/df_aspect_pos.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
